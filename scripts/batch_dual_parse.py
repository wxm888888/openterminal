import os
import glob
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dual_model_parse import dual_model_parse_and_save

def process_single_file(input_file, output_dir, model_a, model_b, judge_model, file_index, total_files):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„è¾…åŠ©å‡½æ•°"""
    filename = os.path.splitext(os.path.basename(input_file))[0]
    output_file = os.path.join(output_dir, f'{filename}_dual.json')
    
    print(f"\n[{file_index}/{total_files}] ğŸ”„ Started: {filename}.txt")
    
    try:
        result = dual_model_parse_and_save(
            input_file=input_file,
            output_file=output_file,
            model_a=model_a,
            model_b=model_b,
            judge_model=judge_model
        )
        
        if result.get('success', False):
            print(f"[{file_index}/{total_files}] âœ… Success: {filename}.txt")
            return ('success', filename, None)
        else:
            reason = result.get('judgment', {}).get('reason', 'Unknown')
            print(f"[{file_index}/{total_files}] âš ï¸ Skipped: {filename}.txt (Reason: {reason[:50]}...)")
            return ('skipped', filename, reason)
    
    except Exception as e:
        error_msg = str(e)
        print(f"[{file_index}/{total_files}] âŒ Failed: {filename}.txt (Error: {error_msg[:50]}...)")
        return ('failed', filename, error_msg)


def batch_process_txt_files(
    input_dir='data/raw/txt',
    output_dir='data/judge',
    model_a='gpt-5.2-2025-12-11',
    model_b='claude-opus-4-5-20251101',
    judge_model='claude-sonnet-4-5-20250929-thinking',
    max_workers=3,
    use_multithreading=True
):
    """
    æ‰¹é‡å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰ txt æ–‡ä»¶ï¼Œå¯¹æ¯ä¸ªæ–‡ä»¶æ‰§è¡Œ dual_model_parse_and_save
    
    Args:
        input_dir: è¾“å…¥ txt æ–‡ä»¶æ‰€åœ¨ç›®å½•
        output_dir: è¾“å‡º JSON æ–‡ä»¶ä¿å­˜ç›®å½•
        model_a: æ¨¡å‹ A åç§°
        model_b: æ¨¡å‹ B åç§°
        judge_model: è£åˆ¤æ¨¡å‹åç§°
        max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤3ï¼Œå»ºè®®2-5ä¹‹é—´ï¼‰
        use_multithreading: æ˜¯å¦ä½¿ç”¨å¤šçº¿ç¨‹ï¼ˆé»˜è®¤Trueï¼‰
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰ txt æ–‡ä»¶
    txt_pattern = os.path.join(input_dir, '*.txt')
    txt_files = sorted(glob.glob(txt_pattern))
    
    if not txt_files:
        print(f"âš ï¸ No .txt files found in {input_dir}")
        return
    
    print(f"{'='*70}")
    print(f"Batch Dual Model Parsing {'(Multithreading)' if use_multithreading else '(Sequential)'}")
    print(f"{'='*70}")
    print(f"Input directory: {input_dir}")
    print(f"Output directory: {output_dir}")
    print(f"Total files: {len(txt_files)}")
    print(f"Model A: {model_a}")
    print(f"Model B: {model_b}")
    print(f"Judge Model: {judge_model}")
    if use_multithreading:
        print(f"Max workers: {max_workers}")
    print(f"{'='*70}\n")
    
    start_time = time.time()
    
    # ç»Ÿè®¡ä¿¡æ¯
    success_count = 0
    failed_count = 0
    skipped_count = 0
    failed_files = []
    skipped_files = []
    
    if use_multithreading:
        # å¤šçº¿ç¨‹å¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(
                    process_single_file,
                    input_file,
                    output_dir,
                    model_a,
                    model_b,
                    judge_model,
                    i,
                    len(txt_files)
                ): input_file
                for i, input_file in enumerate(txt_files, 1)
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_file):
                status, filename, extra_info = future.result()
                
                if status == 'success':
                    success_count += 1
                elif status == 'skipped':
                    skipped_count += 1
                    skipped_files.append((filename, extra_info))
                elif status == 'failed':
                    failed_count += 1
                    failed_files.append((filename, extra_info))
    
    else:
        # é¡ºåºå¤„ç†
        for i, input_file in enumerate(txt_files, 1):
            status, filename, extra_info = process_single_file(
                input_file,
                output_dir,
                model_a,
                model_b,
                judge_model,
                i,
                len(txt_files)
            )
            
            if status == 'success':
                success_count += 1
            elif status == 'skipped':
                skipped_count += 1
                skipped_files.append((filename, extra_info))
            elif status == 'failed':
                failed_count += 1
                failed_files.append((filename, extra_info))
    
    elapsed_time = time.time() - start_time
    
    # æ‰“å°æ±‡æ€»ä¿¡æ¯
    print(f"\n{'='*70}")
    print(f"Batch Processing Summary")
    print(f"{'='*70}")
    print(f"Total files: {len(txt_files)}")
    print(f"âœ… Successful: {success_count}")
    print(f"âš ï¸ Skipped: {skipped_count}")
    print(f"âŒ Failed: {failed_count}")
    print(f"â±ï¸  Total time: {elapsed_time:.2f}s ({elapsed_time/len(txt_files):.2f}s per file)")
    
    if skipped_files:
        print(f"\nSkipped files:")
        for filename, reason in skipped_files[:5]:
            print(f"  - {filename}.txt: {reason[:60]}...")
        if len(skipped_files) > 5:
            print(f"  ... and {len(skipped_files) - 5} more")
    
    if failed_files:
        print(f"\nFailed files:")
        for filename, error in failed_files[:5]:
            print(f"  - {filename}.txt: {error[:60]}...")
        if len(failed_files) > 5:
            print(f"  ... and {len(failed_files) - 5} more")
    
    print(f"{'='*70}")


if __name__ == "__main__":
    # é…ç½®å‚æ•°
    INPUT_DIR = 'data/test'
    OUTPUT_DIR = 'data/results'
    MODEL_A = 'gpt-5.2-2025-12-11'
    MODEL_B = 'claude-opus-4-5-20251101'
    JUDGE_MODEL = 'claude-sonnet-4-5-20250929-thinking'
    
    # å¤šçº¿ç¨‹é…ç½®
    USE_MULTITHREADING = True  # æ˜¯å¦ä½¿ç”¨å¤šçº¿ç¨‹ï¼ˆTrue=å¹¶å‘å¤„ç†ï¼ŒFalse=é¡ºåºå¤„ç†ï¼‰
    MAX_WORKERS = 5  # æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼Œå»ºè®®2-5ï¼ˆå¤ªé«˜å¯èƒ½è¢«APIé™æµï¼‰
    
    # æ‰§è¡Œæ‰¹é‡å¤„ç†
    batch_process_txt_files(
        input_dir=INPUT_DIR,
        output_dir=OUTPUT_DIR,
        model_a=MODEL_A,
        model_b=MODEL_B,
        judge_model=JUDGE_MODEL,
        max_workers=MAX_WORKERS,
        use_multithreading=USE_MULTITHREADING
    )
